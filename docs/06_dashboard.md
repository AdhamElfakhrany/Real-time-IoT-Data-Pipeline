# Step 6 – Dashboard & Monitoring

## Overview
A Streamlit dashboard (`dashboard/app.py`) surfaces core telemetry metrics from the processed datasets generated by the batch ETL job. It provides high-level KPIs, trend charts, and recent alerts to support operations and reporting.

## Prerequisites
- Batch ETL has produced `data/processed/cleansed.parquet` and `aggregates.parquet` (run locally or via `batch-runner` container).
- Python virtualenv activated with the project requirements (`pip install -r requirements.txt`).

## Run the dashboard
```bash
source .venv/Scripts/activate  # or .venv/bin/activate
streamlit run dashboard/app.py
```
The app will read the processed files by default. Use environment variables to customize paths or refresh behavior:
- `PROCESSED_DATA_PATH`
- `DASHBOARD_CLEANSED_PATH`
- `DASHBOARD_AGGREGATES_PATH`
- `DASHBOARD_REFRESH_SECONDS`

## Features
1. **Summary cards** – total events, devices, alert count, devices in alert.
2. **Time-series charts** – temperature & humidity trends over time.
3. **Recent alerts table** – top 25 alerting events with reasons.
4. **Hourly aggregates grid** – per-device/hour metrics from the batch job.

## Extensibility ideas
- Add a live Kafka consumer to show streaming alerts in near real-time.
- Embed Grafana panels or Power BI tiles for richer visuals.
- Persist dashboard snapshots (PNG/PDF) into `docs/` for the final report.
