# Real-time IoT Data Pipeline

Comprehensive, end-to-end project that simulates IoT sensor telemetry, ingests it into streaming infrastructure, and powers both batch ETL analytics and real-time alerting with dashboards.

## Project Goals
- Generate realistic IoT sensor messages (temperature, humidity, status) at configurable intervals.
- Ingest raw events into Kafka (or equivalent) for both streaming and batch consumers.
- Build a structured batch ETL flow that produces curated aggregates (Parquet/SQL).
- Implement a low-latency streaming processor that fires alerts on threshold violations.
- Provide monitoring & dashboards plus a final report summarizing performance.

## Repository Layout
```
.
â”œâ”€â”€ batch/              # Batch ETL scripts (pandas / PySpark)
â”œâ”€â”€ dashboard/          # Streamlit / Grafana configs and assets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/      # Processed outputs (gitignored)
â”‚   â””â”€â”€ raw/            # Landing zone for raw dumps (gitignored)
â”œâ”€â”€ docs/               # Architecture diagrams, final report
â”œâ”€â”€ generator/          # Sensor data generator + utilities
â”œâ”€â”€ infra/              # Docker Compose, IaC, orchestration manifests
â”œâ”€â”€ streaming/          # Streaming jobs (Spark/Flink/ASA)
â”œâ”€â”€ .env.example        # Template for secrets & config
â”œâ”€â”€ requirements.txt    # Python dependencies for core services
â””â”€â”€ README.md           # You are here
```

## Step 1 â€“ Environment Setup
1. **Python environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # or .venv\Scripts\activate on Windows
   pip install -r requirements.txt
   ```
2. **Copy environment template**
   ```bash
   cp .env.example .env
   # populate broker endpoints, thresholds, alert targets
   ```
3. **Local infrastructure** (optional for now)
   - Install Docker Desktop.
   - Ensure ports 9092 (Kafka) and 2181 (Zookeeper) are available.

## Next Steps
- **Milestone 1** âœ”ï¸ `generator/sensor_generator.py` and ingestion targets (stdout/file/Kafka).
- **Milestone 2** âœ”ï¸ pandas batch ETL (`batch/batch_etl.py`) producing cleansed + aggregated datasets.
- **Milestone 3** ğŸš§ streaming processor + alerting sink (`streaming/stream_processor.py`) and Docker stack (`infra/docker-compose.yml`).
- **Milestone 4** ğŸš§ dashboard (`streamlit run dashboard/app.py`) and final report in `docs/`.

Each milestone will be captured in dedicated documentation inside `docs/` along with testing artifacts.
